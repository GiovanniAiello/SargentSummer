%% Preamble
  \documentclass{homework}

  \hwTitle{RMT4 P 2.1 - 2.5} % Assignment title
  \hwDueDate{Sunday,\ July\ 14,\ 2013} % Due date
  \hwClass{Sargent - RA} % Course/class
  \hwAuthor{Spencer Lyon} % Your name

  \usepackage[shortlabels]{enumitem}
  \usepackage{setspace}
  \usepackage{booktabs}

  \providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}

  % Set up listings
  \usepackage{listings}
  \usepackage{color}

  \definecolor{dkgreen}{rgb}{0,0.6,0}
  \definecolor{gray}{rgb}{0.5,0.5,0.5}
  \definecolor{mauve}{rgb}{0.58,0,0.82}

  \lstset{frame=tb,
    language=Python,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=left,
    stepnumber=5,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true
    tabsize=4
  }

%% New Commands I use alot

\begin{document}

\maketitle

\begin{homeworkProblem}[Problem 2.1]

  Consider the Markov Chain $(P, \pi_0) = \left( \left[\begin{smallmatrix} .9 & .1 \\ .3 & .7 \end{smallmatrix}\right], \left[\begin{smallmatrix} .5 \\ .5 \end{smallmatrix}\right]\right)$ and a random variable $y_t = \bar{y}x_t$, where $\bar{y}  = \left[\begin{smallmatrix} 1 \\ 5 \end{smallmatrix}\right]$. Compute the likelihood of the following three histories for $y_t$ for $t = 0, 1 \dots 4$:

  \begin{enumerate}[a.]
    \item $1, 5, 1, 5, 1$
    \item $1, 1, 1, 1, 1$
    \item $5, 5, 5, 5, 5$
  \end{enumerate}

  \vspace{.2in}

  \problemAnswer{

    Each part in this problem is fairly straight-forward. I simply need to pick the correct element of $P$ and multiply them all together with the correct starting value of $\pi_0$.

    \begin{enumerate}[a.]
      \item $P_{21}P_{12}P_{21}P_{12}\pi_{01} = (.3)(.1)(.3)(.1)(.5) = 0.00045$
      \item $P_{11}P_{11}P_{11}P_{11}\pi_{01} = (.9)(.9)(.9)(.9)(.5) = 0.32805$
      \item $P_{22}P_{22}P_{22}P_{22}\pi_{02} = (.7)(.7)(.7)(.7)(.5) = 0.12005$
    \end{enumerate}

    \qed
  }
\end{homeworkProblem}

\begin{homeworkProblem}[Problem 2.2]

  Consider a two-state Markov chain. Consider a random variable $y_t = \bar{y} x_t$ where $\bar{y} = \left[\begin{smallmatrix} 1 \\ 5 \end{smallmatrix}\right]$. It is known that $E(y_{t+1} | x_t) = \left[\begin{smallmatrix} 1.8 \\ 3.4 \end{smallmatrix}\right]$ and that $E(Y^2_{t+1} |x_t) = \left[\begin{smallmatrix} 5.8 \\ 5.4 \end{smallmatrix}\right]$. Find a transition matrix consistent with these conditional expectations. Is this transition matrix unique (i.e. can you find another one that is consistent with these conditional expectations)?

  \vspace{.2in}

  \problemAnswer{

  We will use the content found in section 2.2.6 (Enough one-step-ahead forecasts determine P). In our case we have that $$h = \left[\begin{matrix} 1 & 1 \\ 5 & 25 \end{matrix}\right]$$ and that $$J = \left[\begin{matrix} 1.8 & 5.8 \\ 3.4 & 15.4 \end{matrix}\right]$$ We can now use the expression $P = J h{^-1}$ to solve for P (note that $h{^-1} = \left[\begin{smallmatrix} 5.4 & -1/20 \\ -1/4 & 1/20  \end{smallmatrix}\right]$)

  \begin{align*}
    P &= J h^{-1} \\
      &= \left[\begin{matrix} 1.8 & 5.8 \\ 3.4 & 15.4 \end{matrix}\right] \left[\begin{matrix} 5.4 & -1/20 \\ -1/4 & 1/20  \end{matrix}\right] \\
      &= \left[\begin{matrix} .8 & .2 \\ .4 & .6  \end{matrix}\right]
  \end{align*}

  \qed

  }
\end{homeworkProblem}

\begin{homeworkProblem}[Problem 2.3]

  Consumption is governed by an $n$-state Markov chain $p, \pi_0$ where $P$ is a stochastic matrix and $\pi_0$ is an initial probability distribution. Consumption takes one of the values in the $n x 1$ vector $\bar{c}$. A consumer ranks stochastic processes of consumption $t = 0, 1, \dots $ according to $$ E \sum_{t=0}^{\infty} \beta^t u (c_t)$$ where $E$ is the mathematical expectation and $u(c) = \frac{c^{1 - \gamma}}{1 -\gamma}$ for some parameter $\gamma \ge 1$. Let $u_i = u_i(\bar{c}_i)$. Let $v_i = E \left[\sum_{t=0}^{\infty} \beta^t u(c_t) | x_0 = \bar{e}_i \right]$ and $V = Ev$, where $\beta \in (0, 1)$ is a discount factor.

  \begin{enumerate}[a.]
    \item Let $u$ and $v$ be the $n x 1$ vectors whose $i$th components are $u_i$ and $v_i$, respectively. Verify the following formulas for $v$ and $V$: $v = (I - \beta P)^{-1}u$, and $V = \sum_i \pi_{0, i}v_i$.

    \item Consider the following two markov processes \\
      Process 1: $\pi_0 = \left[\begin{smallmatrix} .5 \\ .5 \end{smallmatrix}\right]$, $P = \left[\begin{smallmatrix} 1 & 0 \\ 0 & 1 \end{smallmatrix}\right]$ \\
      Process 2: $\pi_0 = \left[\begin{smallmatrix} .5 \\ .5 \end{smallmatrix}\right]$, $P = \left[\begin{smallmatrix} .5 & .5 \\ .5 & .5 \end{smallmatrix}\right]$ \\
      For both Markov processes, $\bar{c} = \left[\begin{smallmatrix} 1 \\ 5 \end{smallmatrix}\right]$. \\
      Assume that $\gamma = 2.5$ and $\beta = 0.95$. Compute the unconditional discounted expected utility $V$ for each of these processes. Which of the two processes does the consumer prefer? Redo the calculations for $\gamma = 4$. Now which process does the consumer prefer?

    \item An econometrician observes a sample of 10 observations of consumption rates for our consumer. He knows that one of the two preceding Markov processes generates the data, but he does not know which one. He assigns equal "prior probability" to the two chains. Suppose that the 10 successive observations on consumption are as follows: $1, 1, 1, 1, 1, 1, 1, 1, 1, 1$. Compute the likelihood of this sample under process 1 and under process 2. Denote the likelihood function $\text{Prob(Data|Model}_i), i = 1, 2$.

    \item Suppose that the econometrician uses Bayes' law to revise his initial probability estimates for the two models, where in this context Bayes' law states: $$\text{Prob}(M_i|data) = \frac{\left( \text{Prob(data)}|M_i\right) \cdot \text{Prob}(M_i)}{\sum_j \text{Prob(data)}|M_j \cdot \text{Prob}(M_j)}$$ where $M_i$ denotes model $i$. The denominator of this expression is the unconditional probability of the data. After observing the data sample, what probabilities does the econometrician place on the two possible models?

    \item Repeat the calculation in part d, but now assume that the data sample is $1, 5, 5, 1, 5, 5, 1, 5, 1, 5$.
  \end{enumerate}

  \vspace{.2in}

  \problemAnswer{

      To start this solution, I wish to explicitly write out a few things:

      \begin{itemize}
        \item $c_t = \bar{c}' x_t$
        \item $v_i = E \left[\sum_{t=0}^{\infty} \beta^t u(c_t) | x_0 = \bar{e}_i \right] \rightarrow v_i = E \left[\sum_{t=0}^{\infty} \beta^t u(c_t) | c_0 = \bar{c}_i \right]$
        \item $u = u(\bar{c})$
      \end{itemize}

      \begin{enumerate}[a.]
        % Part a
        \item For this part, I need to write $v$ in vector notation using its component formulas
           \begin{align*}
              v = \left[\begin{matrix} v_1 \\v_2 \\ \vdots \\ v_n \end{matrix}\right]
                 =\left[\begin{matrix}
                            E \left[\sum_{t=0}^{\infty} \beta^t u(c_t) | c_0 = \bar{c}_1 \right] \\
                            E \left[\sum_{t=0}^{\infty} \beta^t u(c_t) | c_0 = \bar{c}_2 \right]\\
                            \vdots \\
                            E \left[\sum_{t=0}^{\infty} \beta^t u(c_t) | c_0 = \bar{c}_n \right] \\
                          \end{matrix}\right]
           \end{align*}

           I will then apply the forecasting function given in section 2.2.5 of the notes (page 35), which reads (note that this function applies because $\beta \in (0, 1)$):

           $$ \sum_{k=0}^{\infty} \beta^k E \left[y_{t+k} |x_t = \bar{e}_i \right] = \left[ (I - \beta P)^{-1}\bar{y}\right] $$

           In this case $y \rightarrow u(c), \bar{y} \rightarrow u(\bar{c}), $ and $ x \rightarrow c$. Making these substitutions we obtain the desired equation: $$ v = (I - \beta P)^{-1} u $$

           The other part of this problem is quite easy. We know that $V$ is defined as $Ev$. To find this expected value we simply need to multiply on the left by the initial probability distribution $\pi_0'$. In mathematical terms we have:

           \begin{align*}
              V = Ev = \pi_0' v = \sum_i \pi_{0, i} v_i
           \end{align*}

           Which is the desired result. \qed

           % Part b
          \item To do this part I simply plugged things into the expressions derived in the previous part of the problem. I did this in the python program found below this problem and got that for both processes 1 and 2 the answers were $V = \begin{cases} -7.26295 & \text{ when } \beta = 2.5 \\ -3.36 & \text{ when } \beta = 4.0 \end{cases}$ Note that under both parameterizations of $\gamma$ the consumer is indifferent between the two consumption processes. \qed

          % Part c
          \item This part is simple. I simple need to use the expression for the likelihood function given in equation 2.2.15: $$ L = \text{ Prob(data| model}_i) = \pi_{0, i_0} \prod_i \prod_j P_{i,j}^{n_ij}$$ In my case this means $\text{ Prob(data| model}_i) = \left[ \pi_{0, 1} \right]_i \left[P_{1, 1}^9 \right]_i$, where $[\cdot]_i$ means for model $i$.

            \begin{itemize}
              \item Applying to process 1: $\text{ Prob(data| model}_1) = \left[ \pi_{0, 1} \right]_1 \left[P_{1, 1}^9 \right]_1 = (0.5) (1)^9 = 0.5$
              \item Applying to process 2: $\text{ Prob(data| model}_2) = \left[ \pi_{0, 1} \right]_2 \left[P_{1, 1}^9 \right]_2 = (0.5) (0.5)^9 = 9.766 e-04$ \qed
            \end{itemize}

          % Part d
          \item This part is simply applying the given expression for Bayes' rule. Because we don't actually know the probability of either model, the solution will be in terms of those probabilities (Prob[$M_i$]). The denominator is the same for both models and is equal to: $$\sum_j \text{Prob(data)}|M_j \cdot \text{Prob}(M_j) = 0.5 \cdot \text{Prob}(M_1) + 9.766 e-04 \cdot \text{Prob}(M_2)$$ The numerators are different for each model, so I will show the final solution for each model separately:

            \begin{itemize}
              \item Model 1:
                \begin{align*}
                  \text{Prob}(M_1|data) &= \frac{\left( \text{Prob(data)}|M_1\right) \cdot \text{Prob}(M_1)}{\sum_j \text{Prob(data)}|M_j \cdot \text{Prob}(M_j)} \\
                    &= \frac{0.5 \cdot \text{Prob}(M_1)}{0.5 \cdot \text{Prob}(M_1) + 9.766 e-04 \cdot \text{Prob}(M_2)}
                \end{align*}
                \item Model 2:
                \begin{align*}
                  \text{Prob}(M_2|data) &= \frac{\left( \text{Prob(data)}|M_2\right) \cdot \text{Prob}(M_2)}{\sum_j \text{Prob(data)}|M_j \cdot \text{Prob}(M_j)} \\
                    &= \frac{9.766 e-04 \cdot \text{Prob}(M_2)}{0.5 \cdot \text{Prob}(M_1) + 9.766 e-04 \cdot \text{Prob}(M_2)}
                \end{align*} \qed
            \end{itemize}

          % Part e
          \item To do this part I need to apply the likelihood function to this new data. For each model I will need to evaluate the following expression (note I just use $P$ here, but I will replace what with the actual $P$ for each model): $$ L = (0.5) \left( P_{12} \right)^4 \left( P_{21} \right)^3 \left( P_{22} \right)^2$$

            \begin{itemize}
              \item Model 1: $L = (0.5) (0)^4 (0)^3 (1)^2  = 0$
              \item Model 2: $L = (0.5) (0.5)^4 (0.5)^3 (0.5)^2  = 9.766 e-04$
            \end{itemize}

          Now I apply Bayes' law:

          \begin{itemize}
              \item Model 1:
                \begin{align*}
                  \text{Prob}(M_1|data) &= \frac{\left( \text{Prob(data)}|M_1\right) \cdot \text{Prob}(M_1)}{\sum_j \text{Prob(data)}|M_j \cdot \text{Prob}(M_j)} \\
                    &= \frac{0 \cdot \text{Prob}(M_1)}{0 \cdot \text{Prob}(M_1) + 9.766 e-04 \cdot \text{Prob}(M_2)} \\
                    &= 0
                \end{align*}
                \item Model 2:
                \begin{align*}
                  \text{Prob}(M_2|data) &= \frac{\left( \text{Prob(data)}|M_2\right) \cdot \text{Prob}(M_2)}{\sum_j \text{Prob(data)}|M_j \cdot \text{Prob}(M_j)} \\
                    &= \frac{9.766 e-04 \cdot \text{Prob}(M_2)}{0 \cdot \text{Prob}(M_1) + 9.766 e-04 \cdot \text{Prob}(M_2)} \\
                    &= \frac{9.766 e-04 \cdot \text{Prob}(M_2)}{ 9.766 e-04 \cdot \text{Prob}(M_2)}  \\
                    &= 1
                \end{align*}
            \end{itemize} \qed

      \end{enumerate}

      \setstretch{0.68}
      \lstinputlisting[language=Python, linerange={1-71}]{../RMT4_Ch2.py}
      \setstretch{1.5}

  }
\end{homeworkProblem}

\begin{homeworkProblem}[Problem 2.4]

  Consider the univariate stochastic process

  \begin{align}
    \label{eq:stoch2.4}
    y_{t+1} = \alpha + \sum_{j=1}^4 \rho_j y_{t+1 - j} + c w_{t+1}
  \end{align}

  where $w_{t+1}$ is a scalar martingale difference sequence adapted to $J_t = \left[ w_t, \dots w_1, y_0, y_{-1}, y_{-2}, y_{-3} \right], \alpha = \mu \left( a - \sum_j \rho_j \right) $ and the $\rho_j$'s are such that the matrix
  $$A = \begin{bmatrix}
  \rho_1 & \rho_2 & \rho_3 & \rho_4 & \alpha \\
  1 &0 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 & 0 \\
   0 & 0 & 1 & 0 & 0 \\
   0 & 0 & 0 & 0 & 1
   \end{bmatrix}$$ has all of its eigenvalues in modulus bounded below unity.

   \begin{enumerate}[a.]
     % Part a: Very similar to example 3 on page 44
     \item Show how to map this process into a first-order linear stochastic difference equation.

     % Part b
     \item For each of the following examples, if possible, assume that the initial conditions are such that $y_t$ is covariance stationary. For each case, state the appropriate initial conditions. Then compute the covariance stationary mean and variance of $y_t$ assuming the following parameter sets of parameter values
       \begin{enumerate}[i.]
         \item $\rho = \left[ 1.2, -0.3, 0, 0 \right], \mu = 10,  c = 1$
         \item $\rho = \left[ 1.2, -0.3, 0, 0 \right], \mu = 10,  c = 2$
         \item $\rho = \left[ 0.9, 0, 0, 0 \right], \mu = 5,  c = 1$
         \item $\rho = \left[ 0.2, 0, 0, 0.5 \right], \mu = 5,  c = 1$
         \item $\rho = \left[ 0.8, 0.3, 0, 0 \right], \mu = 5,  c = 1$
       \end{enumerate}

       \textit{Hint 1:} The Matlab program \texttt{doublej.m}, in particular, the command \texttt{X = doublej(A, C*C')} computes the solution to the matrix equation $AXA' + CC' = X$. This program is on Tom's website.

       \textit{Hint 2:} The mean vector is the eigenvector of $A$ associated with a unit eigen- value, scaled so that the mean of unity in the state vector is unity.

      % Part c
      \item For each case in part b, compute the $h_j$'s in $E_t y_{t+5} = \gamma_0 + \sum_{j=0}^3 h_j Y_{t-j}$.

      % Part d
      \item For each case in part b, compute the $\tilde{h}_j$'s in $E_t \sum_{k=0}^{\infty} 0.95^ky_{t+k} = \sum_{j=0}^3 \tilde{h}_j y_{t-j}$.

      % Part e
      \item For each case in part b, compute the autocovariance $E(y_t - \mu_y)(y_{t-k} - \mu_y)$ for the three values $k = 1, 5, 10$.
   \end{enumerate}

  \vspace{.2in}

  \problemAnswer{

    \begin{enumerate}[a.]

      % Part a
      \item Much of this part was already done for me when the matrix $A$ was given above. I seek an equation in the form of 2.4.1. I show that below:

      \begin{align*}
        \left[\begin{matrix} y_{t+1} \\ y_t \\ y_{t-1} \\ y_{t-2} \\ 1  \end{matrix}\right] &=
        \left[\begin{matrix} \rho_1 &  \rho_2 & \rho_3 & \rho_4 & \alpha \\
        1 &  0 & 0 & 0 & 0 \\
        0 &  1 & 0 & 0 & 0 \\
        0 &  0 & 1 & 0 & 0 \\
        0 &  0 & 0 & 0 & 1 \\
        \end{matrix}\right]
        \left[\begin{matrix} y_{t} \\ y_{t-1} \\ y_{t-2} \\ y_{t-3} \\ 1  \end{matrix}\right] +
        \left[\begin{matrix} c \\  0 \\ 0 \\ 0 \\ 0  \end{matrix}\right] w_{t+1}  \\
        y_{t+1} &= Ay_t + cw_{t+1}
      \end{align*} \qed

      % Part b
      \item To proceed on this part, I need to derive an expression for $\alpha$ in terms of $\rho, \mu, c$. I start with \ref{eq:stoch2.4} and take the unconditional expectation to obtain: $$\mu = \alpha + \mu \sum_{j=1}^4 \rho_j$$ which leads to $$\alpha = \mu - \mu \sum_{j=1}^4 \rho_j$$

        In order for a stochastic process to be covariance stationary, two conditions must hold

        \begin{enumerate}
          \item All eigenvalues of $A$ (except perhaps 1 that corresponds to the constant term and is equal to unity) must be bounded below unity in modulus.
          \item The initial condition $x_0$ comes from a distribution described by the covariance stationary mean and covariance.
        \end{enumerate}

        Now I will proceed with the calculations for each part. Note that I used a python adaptation of the code in \texttt{doublej.m} to compute the solutions as well as some new python code for this problem. They will both be included at the end of this problem.

        \begin{enumerate}[i.]

          % Part 1
          \item The eigenvalues for $A$ are $\lambda = [ 0., 0., 0.355,  0.845, 1.]$, which are all less than 1 in modulus (except the constant term), so there does exist a set of covariance stationary initial conditions. I found that they were equal to:

            \begin{align*}
              \mu = \left[\begin{matrix} 10 \\ 10 \\ 10 \\ 10 \\ 1 \end{matrix}\right]
              \quad & \qquad
              C_x(0) = \begin{bmatrix}
                        7.42857 & 6.85714 & 6 & 5.14286 & 0\\
                        6.85714 & 7.42857 & 6.85714 & 6 & 0\\
                        6 & 6.85714 & 7.42857 & 6.85714 & 0\\
                        5.14286 & 6 & 6.85714 & 7.42857 & 0\\
                        0 & 0 & 0 & 0 & 0\\
                      \end{bmatrix}
            \end{align*}

          % Part 2
          \item The eigenvalues for $A$ are $\lambda = [ 0., 0., 0.355,  0.845, 1.]$, which are all less than 1 in modulus (except the constant term), so there does exist a set of covariance stationary initial conditions. I found that they were equal to:

            \begin{align*}
              \mu = \left[\begin{matrix} 10 \\ 10 \\ 10 \\ 10 \\ 1 \end{matrix}\right]
              \quad & \qquad
              C_x(0) = \begin{bmatrix}
                        29.7143 & 27.4286 & 24 & 20.5714 & 0\\
                        27.4286 & 29.7143 & 27.4286 & 24 & 0\\
                        24 & 27.4286 & 29.7143 & 27.4286 & 0\\
                        20.5714 & 24 & 27.4286 & 29.7143 & 0\\
                        0 & 0 & 0 & 0 & 0\\
                      \end{bmatrix}
            \end{align*}

          % Part 3
          \item The eigenvalues for $A$ are $\lambda = [0., 0., 0., 0.9, 1.]$, which are all less than 1 in modulus (except the constant term), so there does exist a set of covariance stationary initial conditions. I found that they were equal to:

            \begin{align*}
              \mu = \left[\begin{matrix} 5 \\ 5 \\ 5 \\ 5 \\ 1 \end{matrix}\right]
              \quad & \qquad
              C_x(0) = \begin{bmatrix}
                        5.26316 & 4.73684 & 4.26316 & 3.83684 & 0\\
                        4.73684 & 5.26316 & 4.73684 & 4.26316 & 0\\
                        4.26316 & 4.73684 & 5.26316 & 4.73684 & 0\\
                        3.83684 & 4.26316 & 4.73684 & 5.26316 & 0\\
                        0 & 0 & 0 & 0 & 0\\
                      \end{bmatrix}
            \end{align*}

            % Part 4
            \item The eigenvalues for $A$ are $\lambda = [ 0.895, 0.049+0.836i,   0.049-0.836i, -0.795, 1.]$, which are all less than 1 in modulus (except the constant term), so there does exist a set of covariance stationary initial conditions. I found that they were equal to:

            \begin{align*}
              \mu = \left[\begin{matrix} 5 \\ 5 \\ 5 \\ 5 \\ 1 \end{matrix}\right]
              \quad & \qquad
              C_x(0) = \begin{bmatrix}
                        1.4764 & 0.415887 & 0.166355 & 0.241214 & 0\\
                        0.415887 & 1.4764 & 0.415887 & 0.166355 & 0\\
                        0.166355 & 0.415887 & 1.4764 & 0.415887 & 0\\
                        0.241214 & 0.166355 & 0.415887 & 1.4764 & 0\\
                        0 & 0 & 0 & 0 & 0\\
                      \end{bmatrix}
            \end{align*}

            % Part 5
            \item The eigenvalues for $A$ are $\lambda = [ 0., 0., -0.278233,  1.08,  1.]$, which are all not less than 1 in modulus (except the constant term), so there does not exist a set of covariance stationary initial conditions.
        \end{enumerate} \qed

      % Part c
      \item To solve this part of the problem, I set up a matrix equation to represent the expression given in the problem description. The equation I will work with is $$EY = E[X\beta]$$, where $Y = y_{t+5}$, $X = [y_t y_{t-1} y_{t-2} y_{t-3} 1]$ and $\beta$ is a set of coefficients that represents $\gamma_0$ and each of the $h_j$'s.

        I can now obtain the standard linear least squares expression for $\beta$: $$\beta = E\left[(XX')^{-1}\right] E[XY]$$

        To actually get an expression for $\beta$ I need to evaluate the two expectations in the expression above. I do this one at a time.

        \begin{enumerate}
          \item $E\left[(XX')^{-1}\right]$: To evaluate this expression, I first turn to the definition of $C_x(0)$ given just below equation 2.4.10 on page 47:

            \begin{align*}
              C_x(0) &= E\left[\left( x_t - \mu\right) \left(x_t - \mu \right)' \right] \\
                &= E[x_tx_t'] - \mu E[x_t]' - E[x_t]\mu' + \mu\mu' \\
                &= E[x_t x_t'] - \mu\mu'
            \end{align*}

            Now I point out that in this case $x_t = X$, my matrix in the equation above. With this in mind it is clear that the expectation is: $$ E\left[ (XX')^{-1}\right] = \left(C_x(0) + \mu\mu' \right)^{-1}$$

          \item $E[XY]$: For this part I set up a relationship between $X$ and $Y$ in the flavor equation 2.4.3b: $Y_t = X_t'G'$. Using this definition and the result from above, I can evaluate the expectation (note that I need $Y_{t+5} = X_{t+5}' G'$):

            \begin{align*}
              E[XY] &= E[X_t X_{t+5}' G'] \\
                &= E[X_t X_{t-5}'] G' \\
                &= \left(C_x(-5) + \mu\mu' \right) G' \\
                &= \left( C_x(5)' + \mu \mu' \right) G' \\
                &= \left( \left(A^5 C_x(0)  \right)' + \mu \mu' \right) G' \\
                &= \left( C_x(0)' A^{5'} + \mu \mu' \right) G' \\
            \end{align*}

            To evaluate the last few steps I used equation 2.4.11 as well as footnote 7 in RMT4.

        \end{enumerate}

        Now, because I am working with $Y = y_{t+5}$, I identify $G = \left[\begin{matrix} 1 & 0 & 0 & 0 &0 \end{matrix}\right]$. Putting the two expressions for the expectations together I obtain a final expression for $\beta$ that I can evaluate:

        \begin{align*}
          \beta = \left(C_x(0) + \mu\mu' \right)^{-1}  \left( C_x(0)' A^{5'} + \mu \mu' \right) G'
        \end{align*}

        I will apply this expression to each parameterization from part b:

        \begin{enumerate}[i.]
          \item $\beta = \left[\begin{matrix} 0.739 &  -0.260 &  -0.000 &  0.000 &  5.216 \end{matrix}\right]$
          \item $\beta = \left[\begin{matrix} 0.739 &  -0.260 &  -0.000 &  0.000 &  5.216 \end{matrix}\right]$
          \item $\beta = \left[\begin{matrix} 0.590 &  -0.000 &  -0.000 &  0.000 &  2.048 \end{matrix}\right]$
          \item $\beta = \left[\begin{matrix} 0.200 &  0.020 &  0.004 &  0.251 &  2.624  \end{matrix}\right]$
          \item  Not all eigenvalues are less than 1 in modulus, so there is no set of covariance stationary initial conditions, so there is no answer to this problem.
        \end{enumerate} \qed

        % Part d
        \item This problem is very similar to the previous one. The big difference is that on the LHS I have a sum, rather than just$y_{t+5}$. To handle this I use equation 2.4.18 and write the following:

          $$E_t \sum_{j=0}^{\infty} 0.95^j y_{t+j} = G(I - 0.95 A_0)^{-1} x_t $$

          Now I am ready to let $Y = G(I - 0.95 y_0)^{-1} x_t$ and move forward as before. Recall that I had this expression: $$\beta = E\left[(XX')^{-1}\right] E[XY]$$ The first term ($E\left[(XX')^{-1}\right]$) is exactly the same as before, but to obtain the second term I need to insert the new expression for $Y$ (note that when I do this I take the transpose as before):

          \begin{align*}
            E[XY] &= E \left[XX' (I - 0.95A)^{-1'}  G' \right] \\
              &-= E[XX] (I - 0.95A)^{-1'}  G' \\
              &= \left(C_x(0) + \mu\mu' \right) (I - 0.95A)^{-1'}  G'
          \end{align*}

          Putting the two expectations together, I derive the final expression for $\beta$:

          \begin{align*}
            \beta &= E\left[(XX')^{-1}\right] E[XY] \\
              &= \left(C_x(0) + \mu\mu' \right)^{-1} \left(C_x(0) + \mu\mu' \right) (I - 0.95A)^{-1'}  G' \\
              &= (I - 0.95A)^{-1'}  G'
          \end{align*}

          I then just plugged this in to the included python code and let it go to work on each parameter set to obtain beta.

          \begin{enumerate}
            \item $\beta = \left[\begin{matrix} 7.648 &  -2.180 &  0.000 &  0.000 &  145.315 \end{matrix}\right]$
            \item $\beta = \left[\begin{matrix} 7.648 &  -2.180 &  0.000 &  0.000 &  145.315 \end{matrix}\right]$
            \item $\beta = \left[\begin{matrix} 6.897 &  -0.000 &  -0.000 &  0.000 &  65.517 \end{matrix}\right]$
            \item $\beta = \left[\begin{matrix} 2.483 &  1.064 &  1.120 &  1.179 &  70.764  \end{matrix}\right]$
            \item  Not all eigenvalues are less than 1 in modulus, so there is no set of covariance stationary initial conditions, so there is no answer to this problem.
          \end{enumerate} \qed

        % part e
        \item The expectation in part e is $E(y_t - \mu_y)(y_{t-k} - \mu_y)$. This is simply an element of the  autocovariance matrix $C_x(k)$. In fact, it is the (1, 1) element because $C_X(j)$ represents, in this problem, the autocovariance between the vectors $y_t$ and $y_{t-j}$. To evaluate that term I simply used equation 2.4.11, which states: $$C_x(j) = A_0^j C_x(0)$$ Doing the calculation for each parameterization and varying values of $k$ I obtain the results in Table ~\ref{tab:partE2_4}. \qed

    \end{enumerate}

    \setstretch{0.68}
    \lstinputlisting[language=Python, linerange={1-6, 72-235}]{../RMT4_Ch2.py}
    \setstretch{1.5}

  }

  \begin{table}[ht]
    \begin{center}
    \begin{tabular}{lrrrrl}
    \toprule
    Parameterization &         i &         ii &       iii &        iv &    v \\
    \midrule
    k  &           &            &           &           &      \\
    1  &  6.857143 &  27.428571 &  4.736842 &  0.415887 &  NaN \\
    5  &  3.702857 &  14.811429 &  3.107842 &  0.365232 &  NaN \\
    10 &  1.597579 &   6.390317 &  1.835150 &  0.131579 &  NaN \\
    \bottomrule
    \end{tabular}
    \caption{\small The autocovariances from problem 2.4 part e.}
    \label{tab:partE2_4}
    \end{center}
  \end{table}

\end{homeworkProblem}

% TODO: finish solution to this problem
\begin{homeworkProblem}[Problem 2.5]

  A consumer's rate of consumption follows the stochastic process
  \begin{equation}
    \begin{split}
      c_t+1 &= \alpha_c + \sum_{j=1}^2 \rho_j c_{t-j+1} + \sum_{j=1}^2 \delta_j z_{t+1-j} + \psi_1 w_{1, t+1} \\
      z_{t+1} &= \sum_{j=1}^2 \gamma_j c_{t-j+1} + \sum_{j=1}^2 \phi_j z_{t-j+1}   + \psi_2 w_{2, t+1}
    \end{split}
  \end{equation}
  wher $W_{t+1}$ is a $2 x 1$ martingale difference sequence, adapted to $J_t = \left[ w_t \dots w_1 c_0 c_{-1} z_0 z_{-1} \right] $, with contemporaneous covariance matrix $E w_{t+1}w'_{t+1} | J_t = I$, and the coefficients $\pi_j, \delta_j, \gamma_j, phi_j$ are such that the matrix
  $$A = \begin{bmatrix}
  \rho_1 & \rho_2 & \delta_3 & \delta_4 & \alpha_c \\
  1 &0 & 0 & 0 & 0 \\
  \gamma_1 & \gamma_2 & \phi_1 & \phi_2 & 0 \\
   0 & 0 & 1 & 0 & 0 \\
   0 & 0 & 0 & 0 & 1
   \end{bmatrix}$$ has all of its eigenvalues in modulus bounded below unity.

   \begin{equation}
     V_0 = E_0 \sum_{t=0}^{\infty} 0.95^t u-(c_t)
   \end{equation}

   \begin{equation} \label{eq:onePeriodUtil}
     u(c_t) = -0.5(c_t - 60)^2
   \end{equation}

   \begin{enumerate}[a.]
     \item Find a formula $V_0$ interms of the parameters of the one-period utility function (\ref{eq:onePeriodUtil}) and the stochastic process for consumption.
     \item Compute $V_0$ for the following two sets of parameter values:
     \begin{enumerate}[i.]
       \item $\rho=\left[ 0.8 -0.3 \right], \alpha_c =1 \delta = \left[ 0.2 0 \right] , \gamma = \left[ 0 0 \right], \psi = \left[ 0.7 -0.2 \right], \phi_1 = \phi_2 = 1  $
       \item The same as for part i except now $\phi_1 = 2, \phi_2 = 1$.
     \end{enumerate}

     \textit{Hint: } remember \texttt{doublej.m}.
   \end{enumerate}

  \vspace{.2in}

  \problemAnswer{

    \begin{enumerate}[a.]
      \item To begin this part of the problem, I will first plug the expression for the one-period utility function into the equation for $V_0$ and expand/simplify:

        \begin{align*}
          V_0 &= E_0 \sum_{t=0}^{\infty} 0.95^t u-(c_t)\\
            &= E_0 \sum_{t=0}^{\infty} 0.95^t \left[ -0.5 (c_t - 60)^2 \right] \\
            &= E_0 \sum_{t=0}^{\infty} 0.95^t \left[ -0.5 (c_t^2 - 120 c_t - 3600) \right] \\
            &= E_0 \sum_{t=0}^{\infty} 0.95^t \left[ -0.5 c_t^2 + 60 c_t - 1800) \right]
        \end{align*}

        I now expand the summation  from above by separating the $t=0$ terms from all the $t>0$ terms. Doing this allows me to write $V_0$ in the following recursive form:

        \begin{align*}
          V_0 &= E_0 \sum_{t=0}^{\infty} 0.95^t \left[ -0.5 c_t^2 + 60 c_t - 1800) \right] \\
            &= E_0 0.95^0 \left[ -0.5c_0^2 + 60c_0 - 1800 \right] +  E_0 \sum_{t=1}^{\infty} 0.95^t \left[ -0.5 c_t^2 + 60 c_t - 1800) \right] \\
            &= \left[ -0.5c_0^2 + 60c_0 - 1800 \right] +  E_0 V_1
        \end{align*}

        I can express the above expression as a matrix equation:

        \begin{align}
          \label{eq:V0_recursive}
          V_0 = x_0'Gx_0 + 0.95 E_0V_1
        \end{align}

        where

        $$ G = \begin{bmatrix}
                     -0.5 & 0 & 0 & 0 & 30 \\
                     0 & 0 & 0 & 0 & 0 \\
                     0 & 0 & 0 & 0 & 0 \\
                     0 & 0 & 0 & 0 & 0 \\
                     30 & 0 & 0 & 0 & -1800
                   \end{bmatrix}$$

        With this expansion, I see that the $c_t^2$ term and know that  $V_0$ is quadratic in the state vector , so I will assume a general quadratic form for the expression of $V_0$:

        $$ V_0 = x'Rx + \xi $$

        I will eventually need to evaluate the expectation $E_0 V_1$. Before doing this I need to define the stochastic process the form of the canonical stochastic difference equation . For this model, the objects in this equation are defined as follows:

        \begin{align*}
          &x_{t+1} = &A &x_t + &C &w_{t+1} \\
          &\left[\begin{matrix} c_{t+1} \\ c_{t} \\ z_{t+1} \\ z_{t} \\ 1 \end{matrix}\right] =
            &\begin{bmatrix}  % A
             \rho_1 & \rho_2 & \delta_1 & \delta_2 &  \alpha_c \\
             1 & 0 & 0 & 0 & 0 \\
             \gamma_1 & \gamma_2 & \phi_1 & \phi_2 & 0 \\
             0 & 0 & 1 & 0 & 0 \\
             0 & 0 & 0 & 0 & 1
             \end{bmatrix}
             &\left[\begin{matrix} c_t \\ c_{t-1} \\ z_t \\ z_{t-1} \\ 1 \end{matrix}\right] % x
             +
             &\begin{bmatrix} % C
             \psi_1 & 0 \\
             0 & 0 \\
            0 & \psi_2 \\
            0 & 0 \\
            0 & 0
            \end{bmatrix}
            &\begin{bmatrix} % w_{t+1}
              w_{1, t+1}\\
              w_{2, t+1}
            \end{bmatrix}
        \end{align*}

        I will now use the general quadratic form in addition to the matrices defined above to evaluate the expectation $E_0 V_1$ from Equation \ref{eq:V0_recursive}. Note two things:

        \begin{enumerate}[1)]
          \item I use the fact that from the definition of the martingale difference sequence, I know that $E[w_t] = 0$
          \item I borrow from the matrix algebra expressed in section 2.4.5 of RMT4 explained in footnote 9 to move from an expression with $w_1, R, C$ to an expression with the trace of $R, C$.
        \end{enumerate}

        \begin{align*}
          E_0 V_1 &= \xi + x_1' R x_1 \\
            &= \xi + E_0 (A x_0 + C w_1)' R (A x_0 + C w_1) \\
            &= \xi + E_0 x_0'A_0'RAx_0 + E_0 w_1' C' R A x_0 + E_0 x_0' A' R w_1 C + E_0 w_1' C' R C w_1 \\
            &= \xi + x_0'A_0'RAx_0 + 0 + 0 + E_0 w_1' C' R C w_1 \\
            &= \xi + x_0'A_0'RAx_0 + \text{trace}(RCC') \\
        \end{align*}

        I now substitute the expectation back into Equation \ref{eq:V0_recursive} and see

        $$V_0 = x_0' Gx_0 + \xi + x_0'A_0'RAx_0 + \text{trace}(RCC')$$

        I can now set this equal to the general quadratic form, equate terms, and uncover expressions for $R$ and $\xi$.

        \begin{align*}
          V_0 = x'Rx + \xi &= x_0' Gx_0 + \beta \left[ \xi + x_0'A_0'RAx_0 + \text{trace}(RCC') \right] \\
          &\vdots \\
            R &= G + \beta[A'RA]\\
            d &= \frac{\beta}{1- \beta} \text{trace}(C'RC)
        \end{align*}

        Substituting these expressions back into the general quadratic form yields the desired expression for $V_0$. \qed

      \item This part of the problem requires that I use the routine \texttt{doublej(sqrt(0.95) * A, G)} to obtain $B$. I can then plug this expression in to determine the value of $d$ do this for each parameterization and show the results below:

      \begin{enumerate}
        \item $$ R = \begin{bmatrix}
                             $-1.328\e{+05}$ & $-1.280\e{+05}$ & $0.000\e{+00}$ & $0.000\e{+00}$ & $-6.690\e{+04}$\\
                             $-1.280\e{+05}$ & $-1.262\e{+05}$ & $0.000\e{+00}$ & $0.000\e{+00}$ & $-6.356\e{+04}$\\
                             $0.000\e{+00}$ & $0.000\e{+00}$ & $0.000\e{+00}$ & $0.000\e{+00}$ & $0.000\e{+00}$\\
                             $0.000\e{+00}$ & $0.000\e{+00}$ & $0.000\e{+00}$ & $0.000\e{+00}$ & $0.000\e{+00}$\\
                             $-6.690\e{+04}$ & $-6.356\e{+04}$ & $0.000\e{+00}$ & $0.000\e{+00}$ & $-3.600\e{+04}$\\
                           \end{bmatrix}
                $$

                $$
                \xi = -2.524\e{+06}
                $$
          \item $$ R = \begin{bmatrix}
                             $-1.328\e{+05}$ & $-1.280\e{+05}$ & $0.000\e{+00}$ & $0.000\e{+00}$ & $-6.690\e{+04}$\\
                             $-1.280\e{+05}$ & $-1.262\e{+05}$ & $0.000\e{+00}$ & $0.000\e{+00}$ & $-6.356\e{+04}$\\
                             $0.000\e{+00}$ & $0.000\e{+00}$ & $0.000\e{+00}$ & $0.000\e{+00}$ & $0.000\e{+00}$\\
                             $0.000\e{+00}$ & $0.000\e{+00}$ & $0.000\e{+00}$ & $0.000\e{+00}$ & $0.000\e{+00}$\\
                             $-6.690\e{+04}$ & $-6.356\e{+04}$ & $0.000\e{+00}$ & $0.000\e{+00}$ & $-3.600\e{+04}$\\
                           \end{bmatrix}
                $$

                $$
                \xi = -1.010e+07
                $$

      \end{enumerate} \qed

    \end{enumerate}

    \setstretch{0.68}
    \lstinputlisting[language=Python, linerange={1-6,237-322}]{../RMT4_Ch2.py}
    \setstretch{1.5}

  }
\end{homeworkProblem}

\end{document}
