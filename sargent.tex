% Created 2013-07-10 Wed 01:34
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\providecommand{\alert}[1]{\textbf{#1}}

\title{sargent}
\author{Spencer Lyon}
\date{\today}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs Org-mode version 7.9.3f}}

\begin{document}

\maketitle

\setcounter{tocdepth}{4}
\tableofcontents
\vspace*{1cm}

\section{RMT4}
\label{sec-1}
\subsection{Chapter 2}
\label{sec-1-1}
\subsubsection{Markov Chains}
\label{sec-1-1-1}
\paragraph{Definitions}
\label{sec-1-1-1-1}


\begin{itemize}
\item The \textbf{Markov Property} is that the conditional probability
  $P(x_t|x_{t-1},x_{t-2}, \dots) = P(x_t|x_{t-1})$
\item A matrix is \textbf{stochastic} if two things hold: 1. $P_{ij} >= 0 \forall
  i, j$ and 2. $\sum_j P_{ij}=1 \forall i$
\item A \textbf{Markov Chain} is made up of 3 things: 1. the standard basis
  vectors $e_i$, 2. transition matrix $P$, where $P_{ij}$ that defines
  the probability of moving from state $i$ to state $j$.
\item The probability of moving from state $i$ to state $j$ in $k$ periods
  is $P^{(k)}_{ij}$. Note that the parenthesis are there to
  disambiguate what object the power $k$ is operating on. I mean it to
  say the $ij$ element of the matrix $P$ raised to the $k$ power, not
  the $ij$ element of $P$ raised to the $k$ power.
\item An \textbf{unconditional probability} is a $(1 x n)$ vector where element
  $i$ is the probability that $x_t = x_i$. It is indexed over time and
  evolves by multiplying on the right by $P$. ($\pi_t = \pi_{t-1}P$)
\item A distribution is \textbf{stationary} if $\pi_{t+1}=\pi_t$, or in other
  words $\pi' = \pi' P \rightarrow (I-P')\pi = 0$. This makes $\pi$ an
  eigenvector of $P'$ corresponding to a unit eigenvalue. (Normally we
  have $(I\lambda - P')$, but in this case $\lambda=1$). For all
  stochastic matrices $P$, there exists at least one stationary
  distribution. This distribution is unique iff there is only one unit
  eigenvalue.
\item An \textbf{absorbing state} is a state in a stationary distribution that
  has a probability of 1. This means there can only be one absorbing
  state in every stationary distribution. An \textbf{absorbing subset} is a
  subset of the entire state that takes up the entire
  probability. Again, there can only be one absorbing subset, but this
  subset can contain many items.
\item A process is \textbf{asymptotically stationary} if, in the limit as $t
  \rightarrow \infty$ the unconditional distribution $\pi_t
  \rightarrow \pi_{\infty}$. If this $\pi_{\infty}$ is the same
  regardless of the starting value $\pi_0$, then the process is said
  to be asymptotically stationary with a unique invariant
  distribution.
\item The \textbf{law of iterated expectations} is illustrated in the fact that
  when $y_{t+1} = \bar{y}' P' x_t + \bar{y}'v_{t+1}$ ($x$ is state,
  $y$ is a function of the state, $P$ is transition matrix, $v$ is
  random shock with $E[v] = 0$) we have that
  $E[E[Y_{t+1}|x_{t+1}]|x_t] = E[y_{t+1}|x_t]$. More formally, this
  law says that for any random variable $z$ and two information sets
  $J$, $I$ with $j \subset I$, \$E[E(z|I)]|J] = E(z|J)
\begin{itemize}
\item Example: $$
    \begin{align}
    E y_1 &= \sum_j \pi_{1, j} \bar{y}_j = \pi_1' \bar{y} - (\pi_0'P)\bar{y} = \pi_0' (P \bar{y}) \\
    E[E(y_1|x_0 = e_i)] &= \sum_i \pi_{0, i} \sum_j P_{ij}\bar{y}_j = \sum_j \left(\pi_{0, i} P_{ij} \right)\bar{y}_j = \pi_1' \bar{y} = E y_1
    \end{align}
    $$
\end{itemize}
\item A random variable is said to be \textbf{invariant} if it $y_t = y_0, t \ge
  0$, for all realizations of $x_t, t \ge 0$ that occur with positive
  probability under $(P, \pi)$. In other words, the random variable
  $y_t$ remains constant at $y_0$, even while the underlying state
  $x_t$ moves through the state space $X$.
\item Any stochastic process $y_t$ that follows the rule $E[y_{t+1}|x_t] =
  y_t$ is said to be a \textbf{martingale}.
\item A stationary Markov chain is said to be \textbf{ergodic} if the only
  invariant functions $\bar{y}$ are constant with probability 1 under
  the stationary unconditional probability distribution $\pi$,
  i.e. $\bar{y}_i = \bar{y}_j \forall i, j$ with $\pi_i >0$ and $\pi_j
  >0$. Another definition I keep seeing online is that a Markov chain
  is ergodib or irreducible if it is possible to eventually get from
  every state to every other state with positive probability.
\item The \textbf{likelihood} for a stochastic matrix following the Markov
  property can be written as follows $$L = \pi_{0, i_0} \prod_i
  \prod_j P_{i, j}^{n_{ij}}$$ where $n_{ij}$ is the number of times a
  one period transition from state $i$ to state $j$ occurs. This
  function is classified as a \emph{multinomial} distribution.
\end{itemize}
\paragraph{Algorithms and Applications}
\label{sec-1-1-1-2}
\begin{itemize}

\item Finding P from one step ahead conditional expectations\\
\label{sec-1-1-1-2-1}%
If the transition matrix $P$ is unknown, but you can determine
     conditional expectations of $n$ independent functions (i.e., $n$
     linearly independent vectors $h_1, \dots, h_n$), you will have
     uniquely identified $P$.


\item Eigenvalues and left/right eigenvectors of stochastic matrices\\
\label{sec-1-1-1-2-2}%
The unit eigenvalues of $P$ have left-eigenvectors that are the
     stationary distributions of the chain [ $(I - P')\pi = 0$ ] and
     right eigenvectors that are invariant functions of the chain [
     $(P-I) \bar{y} = 0$ ]

     The left eigenvectors of a matrix $A$ are found by solving $(I -
     A') x = 0$ for $x$. They are the exact same as the right
     eigenvectors of $A'$ (right eigenvectors are what I have been
     finding my whole life)


\item Finding stationary distributions\\
\label{sec-1-1-1-2-3}%
To find the stationary distributions of a Markov chain $P$,
     simply find the left-eigenvectors (right eigenvectors of $P'$)
     and normalize so that is sums to 1 (find it in terms of simple
     whole numbers and divide the vector by its sum). To find the
     invariant functions just find the right eigenvectors of $P$ and
     then you can multiply out front by any scalar because they don't
     have to be normalized to 1.


\item Markov chain parameter estimation\\
\label{sec-1-1-1-2-4}%
Estimation for free parameters $\theta$ of a Markov process: Let
     the transition matrix $P$ and the initial distribution $\pi_0$ be
     functions $P(\theta), \pi_0(\theta)$ of a vector of free
     parameters $\theta$. Given a sample $\{x_t\}_{t=0}^T$, regard the
     likelihood function as a function of the parameters $\theta$. As
     the estimator of $\theta$, choose the value that maximizes the
     likelihood function $L$ (just a very verbose way of saying to
     MLE).

\end{itemize} % ends low level
\paragraph{Theorems}
\label{sec-1-1-1-3}


\begin{itemize}
\item Let $\bar{y}$ define a random variable as a function of an
  underlying state $x$, where $x$ is governed by a stationary Markov
  chian ($P, \pi$). Then $$ \frac{1}{T} \sum_{t=1}^T y_t \rightarrow
  E[y_{\infty} |x_0]$$ with probability 1.
\item Let $(P, \pi)$ be a stationary Markov chain. If $$E[y_{t+1}|X+t] =
  y_t$$ then the random variable $y_t = \bar{y}' x_t$ is invariant.
\item Let $\bar{y}$ define a random variable on a stationary and ergodic
  Markov chain $(P, \pi)$. Then $$ \frac{1}{T} \sum_{t=1}^T y_t
  \rightarrow E[y_0]$$ with probability 1. In other words, the time
  series average converges t the population mean of the stationary
  distribution.
\end{itemize}
\subsubsection{Continuous-state Markov chain}
\label{sec-1-1-2}
\paragraph{Definitions}
\label{sec-1-1-2-1}

\begin{itemize}
\item \textbf{State transitions} are defined the by the cumulative distribution
  function (cdf) $$\Pi(s'|s) = \text{Prob}(s_{t+1} \ge s'|s_t=s)$$ The
  initial state $s_0$ is given by the cdf $$\Pi_0(s) = \text{Prob}(s_0
  \le s)$$
\item The \textbf{transition density} is $$\pi(s'|s) = \frac{\partial}{\partial
  s'} \Pi(s'|s)$$ and the initial density is $$\pi_0(s) =
  \frac{\partial}{\partial s} \Pi_0(s) $$
\item A \textbf{history} is given the notation $s^t = [s_t, s_{t-1}, \dots, s_0]$
  and is just a vector of the value of a variable over time.
\item A Markov chain is \textbf{stationary} if $\pi_0$ satisfies ($\forall s \in
  S$) $$\pi_0(s') = \int_s \pi(s'|s) \pi_0(s) ds$$
\item A function $\phi$ of a Markov chain is invariant if $$ \int \phi(s')
  \pi(s'|s)ds' = \phi(s)$$
\end{itemize}
\paragraph{Theorems}
\label{sec-1-1-2-2}


\begin{itemize}
\item Let $y(s)$ be a random variable, a measurable function of $s$, and
  let ($\pi(s'|s), \pi_0(s)$) be a stationary and ergodic
  continuous-state Markov process. Assume that $E|y| < +\infty$. Then
  $$ \frac{1}{T} \sum_{t=1}^T y_t = Ey = \int y(s) \pi_0(s) ds$$ with
  probability 1 with respect to the distribution $\pi_0$.
\end{itemize}
\subsubsection{Stochastic linear difference equations}
\label{sec-1-1-3}
\paragraph{Definitions}
\label{sec-1-1-3-1}


\begin{itemize}
\item A \textbf{martingale difference sequence adapted to J$_t$} is a sequence
  $z_{t+1}$ that satisfies the equation $E[z_{t+1}|J_t] = z_t$
\item The \textbf{first order stochastic linear difference equation} is of the
  following form $x_{t+1} = A_0 x_t + C w_{t+1}$. $w_{t+1}$ must
  satisfy one of 3 assumptions (in order of decreasing
  strictness. Note that if a higher one is satisfied, all lower ones
  are too. i.e. if A1 then A2 and A3, if A2 then A3):
\begin{itemize}
\item Distributed i.i.d $N \sim (0, I)$
\item $Ew_{t+1}|J_t = 0$ and $Ew_{t+1}w_{t+1}'|J_t = I$, where $J_t$ is
    all the information at time $t$ and the $E[\cdot |J_t]$ denotes a
    conditional expectation.
\item $E w_{t+1} = 0$ and $Ew_{t+1}w_{t-j}' = I$ when $j=0$ and
    $Ew_{t+1}w_{t-j}' = 0$ when $j \ne 0$ (this is white noise)
\end{itemize}
\item A \textbf{stochastic process} is a sequence of random vectors
\item A stochastic process $\{x_t\}$ is said to be \textbf{covariance stationary}
  if it satisfies the following two properties: (a) the mean is
  independent of time ($Ex_t = E x_0 \forall t$) and (b) the sequence
  of autocovariance matrices $E(x_{t+j} - Ex_{t+j})(x_t - E x_t)'$
  depends on the separation between dates $j$, but not on $t$.
\item A square real valued matrix $A_0$ is said to be stable if all of its
  eigenvalues in modulus are strictly less than unity.
\end{itemize}
\paragraph{Algorithms and Applications}
\label{sec-1-1-3-2}
\begin{itemize}

\item Linear Stochastic Difference equation form\\
\label{sec-1-1-3-2-1}%
To put a stochastic process in the form of a first order linear
stochastic difference equation, come up with matrices $A_0$, $C$ and
(optionally) $G$ that satisfy $$x_{t+1} = A_0 x_t + C w_{t+1}$$ $$ y_t
= G x_t$$


\item Covariance stationary stochastic processes\\
\label{sec-1-1-3-2-2}%
Whether or not a stochastic process is covariance stationary often
 depends on the form in which the process is presented and some
 initial conditions. We will be working with the form:
 \$\left[\begin{smallmatrix} x$_{\mathrm{1, t+1}}$ \\ x$_{\mathrm{2, t+1}}$
 \end{smallmatrix}\right] = \left[\begin{smallmatrix} 1 \& 0 \\ 0 \&
 \~{}{A\} \end{smallmatrix}\right] \left[\begin{smallmatrix} x$_{\mathrm{1, t}}$
 \\ x$_{\mathrm{2t}}$ \end{smallmatrix}\right] + \left[\begin{smallmatrix} 0
 \\ \tilde\{C\} \end{smallmatrix}\right] w$_{\mathrm{t+1}}$\$. In this form we
 assume that $\tilde{A}$ is a stable matrix (so that 1 cannot be an
 eigenvalue of $\tilde{A}$)

To find the covariance stationary initial conditions for the mean and
covariance of a stochastic process, you must follow these steps:

\begin{enumerate}
\item Set your system up in the form described above.
\item Solve for the stationary mean by taking expected values of both
   sides of the stochastic linear difference equation and ending up
   with this equation: $(I - A_i) \mu = 0$. You then need to solve for
   the eigenvector that corresponds to the single unit eigenvalue of
   $A_0$. This vector is the stationary mean vector, or $\mu$.
\item Solve for the stationary variance by solving the matrix quadratic:
   $C_x(0) = A_0 C_x(0)A_0' + CC'$. The autocovariance process through
   time can be found via the relation: $C_x(j) = A_0^j C_x(0)$
\end{enumerate}


\item To solve for impulse response functions,\\
\label{sec-1-1-3-2-3}%
Re-write the stochastic linear difference equation using the lag
operator $Lx_{t+1} = x_t$ to get $(I - A_o L)x_{t+1} = C
w_{t+1}$. Iterating forward from time $t=0$ leads to the following
expressions for $x_t$ and $y_t$: $$x_t = A_0^t x_0 + \sum_{j=0}^{t-1}
A_0^j C W_{j-t}$$ $$ y_t = G A_0^t x_0 + G \sum_{j=0}^{t-1} A_o^j C
w_{t-j}$$ The impulse response function for $x$ is $h_j = A_0^j C$ and
the impulse response function for $y$ is $\tilde{h}_j = G A_0^j C$


\item Forecasting the conditional covariance matrix\\
\label{sec-1-1-3-2-4}%
Using the impulse response functions from above we can forecast the
expected $t$ period ahead conditional covariance matrix $E(y_t -
EY_t|x_0)(y_t - Ey_t|x_0)' = G \left[\sum_{h=0}^{t-1} A_0^h C C'
{A_0^h}'\right] G'$


\item How to apply the Howard improvement algorithm to the evaluation of dynamic criterion.\\
\label{sec-1-1-3-2-5}%
We will be working with the following equations: $$x_{t+1} = A x_t + B
u_t + C w_{t+1}$$ $$u_t = - F_0 x_t$$ $$v(x_0) = - E_0
\sum_{t=0}^{\infty} \beta^t \left[x_t'Rx_t + u_t'Qu_t \right]$$

\begin{enumerate}
\item Start with some given policy rule $F_0$ and use it to find $P_0 =
   R + F_0'QF_0 + \beta(A - B F_0)'P_0(A - BF_0)$
\item Use this $P_0$ to find a $F_1 = \beta(Q + \beta B'P_0B)^{-1}B'P_0A$
\item Repeat this sequence using the expressions $P_j = R + F_j' Q F_j +
   \beta(A - B F_j)'P_j(A - BF_j)$ and \$F$_{\mathrm{j+1}}$ = $\beta$(Q + $\beta$ B'
   P$_j$ B)$^{\mathrm{-1}}$B'P$_j$
\end{enumerate}
\end{itemize} % ends low level
\subsubsection{Population}
\label{sec-1-1-4}
\paragraph{Algorithms and Applications}
\label{sec-1-1-4-1}
\begin{itemize}

\item Parameter estimation\\
\label{sec-1-1-4-1-1}%
This is simple least squares. If $Y$ is governed by a
      state-space system and somehow $X$ comes from $Y$. you can do
      least squares on them to get a vector $\beta$ that minimizes the
      sum of squared errors for the regression. We get that $$\beta =
      (EYX')[E(XX')]^{-1}$$

\item Multiple Regressors\\
\label{sec-1-1-4-1-2}%
If instead of being a scalar $Y$ is a vector of random
      variables, then you will do multiple regressions. In this case
      $\beta$ becomes a matrix and the error term is a vector. The
      equation for beta is found in the same way.
\end{itemize} % ends low level
\subsubsection{Estimation of Model Parameters}
\label{sec-1-1-5}
\paragraph{Algorithms and Applications}
\label{sec-1-1-5-1}
\begin{itemize}

\item Likelihood function\\
\label{sec-1-1-5-1-1}%
The Likelihood function is defined as the joint probability
      distribution of all the state variables $f(x_t, x_{t-1}, \dots
      x_0)$. This distribution can be factored by multiplying
      successive conditional joint probability distributions $$f(x_t,
      x_{t-1}, \dots, x_0) = f(x_t |x_{t-1}, \dots, x_0)
      f(x_{t-1}|x_{t-2}, \dots, x_0) \dots f(x_1|x_0)f(x_0)$$ Note
      that for a Markov system the equation becomes $f(x_t|x_{t-1},
      \dots, x_0) = f(x_t|x_{t-1})$ because of the Markov
      property. This means that the likelihood function becomes
      $$f(x_t, x_{t-1}, \dots, x_0) = f(x_t |x_{t-1})
      f(x_{t-1}|x_{t-2})\dots f(x_1|x_0)f(x_0)$$

\item Special log-likelihood function\\
\label{sec-1-1-5-1-2}%
If the $w$'s underlying the stochastic process for $Y$ are
      Gaussian, then we know what the conditional distribution
      $f(x_{t+1}|x_t)$ is Gaussian with mean $A_0x_t$ and covariance
      matrix $CC'$. Taking the log of the conditional density of the
      $n$ dimensional vector $x_t$ becomes $$\log f(x_{t+1}|x_t) =
      -0.5n \log(2 \pi) - 0.5 \log \text{det}(CC') - 0.5(x_{t+1} -
      A_0x_t)'(CC')^{-1}(x_{t+1} - A_0x_t)$$
\end{itemize} % ends low level
\subsubsection{The Kalman filter}
\label{sec-1-1-6}
\paragraph{Definitions}
\label{sec-1-1-6-1}
\paragraph{Algorithms and Applications}
\label{sec-1-1-6-2}
\paragraph{Theorems}
\label{sec-1-1-6-3}
\section{\textbf{TODO} Marching Orders, number 1 [0/4]}
\label{sec-2}


Clock summary at \textit{2013-07-10 Wed 01:25}


\begin{center}
\begin{tabular}{llr}
 Headline                                        &  Time            &         \\
\hline
 \textbf{Total time}                             &  \textbf{20:50}  &         \\
\hline
 TODO [\#A] Marching Orders, number 1\ldots{}    &  20:50           &         \\
 \hspace{3ex} TODO [\#A] Read Chapter 2 of RMT4  &                  &  15:17  \\
 \hspace{3ex} TODO [\#A] Work exercises 2.1-2.5  &                  &   5:33  \\
\end{tabular}
\end{center}



  \texttt{DEADLINE:} \textit{2013-07-14 Sun}
\subsection{\textbf{TODO} Read Chapter 2 of RMT4}
\label{sec-2-1}


I should probably look over section 2.4.5.2 again. It was a bit
complicated and I couldn't replicate its results on my own.
\subsection{\textbf{TODO} Read the two techincal appendixes}
\label{sec-2-2}
\subsection{\textbf{TODO} Work exercises 2.1-2.5}
\label{sec-2-3}
\subsection{\textbf{TODO} Think of python examples \textbf{:TOM:}}
\label{sec-2-4}


Re-create \texttt{markov.m} and other Matlab programs

\end{document}
